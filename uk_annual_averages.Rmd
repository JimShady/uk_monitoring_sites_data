---
title: "UK Annual Averages"
author: "James David Smith"
date: "26 February 2019"
output: html_document
---

```{r setup, include=FALSE}
rm(list = ls())

library(tidyverse)
library(openair)
library(lubridate)
library(gganimate)
library(sf)

```

Import air quality data from KCL and calculate annual means + completeness by site code and pollutant.

```{r, message=F, warning=F, echo=F}

kcl           <- importMeta(source = 'kcl') %>% group_by(code) %>% summarise() %>% mutate(code = as.character(code))

kcl_data      <- list()

for (i in 1:nrow(kcl)) {
  
  kcl_data[[i]] <- importKCL(site = kcl$code[i], pollutant = 'all', year = 2013:2019)
  
  closeAllConnections()
  
}

kcl_data[sapply(kcl_data, is.null)] <- NULL

result_data <- unique(unlist(lapply(kcl_data, names))) %>% 
                as_tibble() %>% 
                rownames_to_column() %>% 
                spread(value, rowname, fill=NA) %>%
                mutate(site = as.character(site))

result_data[1,] <- NA

for (i in 1:length(kcl_data)) {
  
  result_data <- suppressWarnings(bind_rows(as_tibble(kcl_data[[i]]), result_data))
  
}

rm(kcl_data)

kcl_means <- result_data %>% 
              select(-site, -pm10_raw, -so2, -v10, -v2.5, -nv10, -nv2.5) %>% 
              mutate(date = year(date)) %>%
              group_by(date, code) %>% 
              summarise_all(funs(count = sum(!is.na(.)), 
                                 mean  = mean(., na.rm=T)))

# Calculate numbers of days of exceedance per site. Decided need to have >= 19 hours of data in a day for the mean to be valid
# 
kcl_pm10days <- result_data %>%
                  select(date, code, pm10) %>%
                  group_by(format(date, '%Y-%m-%d'), code) %>%
                  summarise_all(funs(count = sum(!is.na(.)), 
                                 mean  = mean(., na.rm=T))) %>%
                  rename(day = `format(date, "%Y-%m-%d")`) %>%
                  select(day, code, pm10_count, pm10_mean) %>%
                  filter(pm10_count >= 17 & pm10_mean > 50) %>%
                  select(day, code, pm10_mean) %>%
                  group_by(substr(day,1,4), code) %>%
                  summarise(exceedance_days = n()) %>%
                  ungroup() %>%
                  rename(date         = `substr(day, 1, 4)`,
                         mean         = exceedance_days,
                         measurements = NA) %>%
                  mutate(pollutant  = 'pm10d',
                         date       = as.numeric(date),
                         hours      = NA,
                         percentage = NA)
                  
  
counts <- kcl_means %>% 
  select(date, code, nox_count, no2_count, o3_count, pm10_count, co_count, pm25_count) %>% 
  gather(pollutant, measurements, nox_count, no2_count, o3_count, pm10_count, co_count, pm25_count) %>%
  mutate(pollutant = gsub("_.*", "", pollutant))


means <- kcl_means %>% 
  select(date, code, nox_mean, no2_mean, o3_mean, pm10_mean, co_mean, pm25_mean) %>% 
  gather(pollutant, mean, nox_mean, no2_mean, o3_mean, pm10_mean, co_mean, pm25_mean) %>%
  mutate(pollutant = gsub("_.*", "", pollutant))

rm(result_data)

kcl_result <- bind_cols(means, counts) %>% #left_join
          select(date, code, pollutant, mean, measurements) %>%
          filter(!is.na(mean)) %>% 
          ungroup()

hours <- tibble(year  = 2013:2019,
                hours = c(8761,8761,8761,8785,8761,8761,8761))

kcl_result            <- left_join(kcl_result, hours, by = c('date' = 'year'))
kcl_result$percentage <- 100*(kcl_result$measurements / kcl_result$hours)

kcl_result    <- bind_rows(kcl_result, kcl_pm10days)

rm(counts, hours, kcl, kcl_means, means, kcl_pm10days)

```

Import air quality data from AURN and calculate annual means + completeness by site code and pollutant.

```{r, message=F, warning=F, echo=F}

aurn           <- importMeta(source = 'aurn') %>% group_by(code) %>% summarise() %>% mutate(code = as.character(code))

aurn_data      <- list()

for (i in 1:nrow(aurn)) {
  
  aurn_data[[i]] <- importAURN(site = aurn$code[i], pollutant = 'all', year = 2013:2019)
  
  closeAllConnections()
  
}

aurn_data[sapply(aurn_data, is.null)] <- NULL

result_data <- unique(unlist(lapply(aurn_data, names))) %>% 
                as_tibble() %>% 
                rownames_to_column() %>% 
                spread(value, rowname, fill=NA) %>%
                mutate(site = as.character(site))

result_data[1,] <- NA

for (i in 1:length(aurn_data)) {
  
  result_data <- suppressWarnings(bind_rows(as_tibble(aurn_data[[i]]), result_data))
  
}

rm(aurn_data)

aurn_means <- result_data %>% 
              select(-site, -wd, -ws,-nv10,-v10,-nv2.5,-v2.5) %>% 
              mutate(date = year(date)) %>%
              rename(pm25 = pm2.5) %>%
              group_by(date, code) %>% 
              summarise_all(funs(count = sum(!is.na(.)), 
                                 mean  = mean(., na.rm=T)))

# Calculate numbers of days of exceedance per site. Decided need to have >= 19 hours of data in a day for the mean to be valid
# 
aurn_pm10days <- result_data %>%
                  select(date, code, pm10) %>%
                  group_by(format(date, '%Y-%m-%d'), code) %>%
                  summarise_all(funs(count = sum(!is.na(.)), 
                                 mean  = mean(., na.rm=T))) %>%
                  rename(day = `format(date, "%Y-%m-%d")`) %>%
                  select(day, code, pm10_count, pm10_mean) %>%
                  filter(pm10_count >= 17 & pm10_mean > 50) %>%
                  select(day, code, pm10_mean) %>%
                  group_by(substr(day,1,4), code) %>%
                  summarise(exceedance_days = n()) %>%
                  ungroup() %>%
                  rename(date         = `substr(day, 1, 4)`,
                         mean         = exceedance_days) %>%
                  mutate(pollutant  = 'pm10d',
                         date       = as.numeric(date),
                         hours      = NA,
                         measurements = NA,
                         percentage = NA)
                  
  
counts <- aurn_means %>% 
  select(date, code, nox_count, no2_count, o3_count, pm10_count, co_count, pm25_count) %>% 
  gather(pollutant, measurements, nox_count, no2_count, o3_count, pm10_count, co_count, pm25_count) %>%
  mutate(pollutant = gsub("_.*", "", pollutant))


means <- aurn_means %>% 
  select(date, code, nox_mean, no2_mean, o3_mean, pm10_mean, co_mean, pm25_mean) %>% 
  gather(pollutant, mean, nox_mean, no2_mean, o3_mean, pm10_mean, co_mean, pm25_mean) %>%
  mutate(pollutant = gsub("_.*", "", pollutant))

rm(result_data)

aurn_result <- bind_cols(means, counts) %>% #left_join
          select(date, code, pollutant, mean, measurements) %>%
          filter(!is.na(mean)) %>% 
          ungroup()

hours <- tibble(year  = 2013:2019,
                hours = c(8761,8761,8761,8785,8761,8761,8761))

aurn_result            <- left_join(aurn_result, hours, by = c('date' = 'year'))
aurn_result$percentage <- 100*(aurn_result$measurements / aurn_result$hours)

aurn_result    <- bind_rows(aurn_result, aurn_pm10days)

rm(counts, hours, aurn, aurn_means, means, aurn_pm10days)

```

Combine data

```{r}

result <- bind_rows(kcl_result, aurn_result)
rm(kcl_result, aurn_result,i)

result <- select(result, -measurements, -hours) %>% rename(capture_rate = percentage)

```

Identify London sites

```{r}

box   <- st_read('laei_box.geojson',quiet=TRUE) %>% mutate(name = 'LAEIBox')

sites <- bind_rows(importMeta(source = 'kcl'), importMeta(source = 'aurn')) %>% 
          as_tibble() %>% 
          dplyr::select(code, latitude, longitude, site.type) %>% 
          st_as_sf(coords = c('longitude', 'latitude'), na.fail = F) %>%
          st_set_crs(4326)

sites <- st_join(sites, box, join = st_intersects)

rm(box)

result <- left_join(result, sites) %>% st_as_sf() %>% st_transform(27700)

result[is.na(result$name), 'name'] <- 'Non-London'

```

Count sites per year

```{r}

sites_per_year <- result %>% 
                    as_tibble() %>% 
                    group_by(pollutant, date) %>% 
                    summarise(n()) %>% 
                    ungroup() %>% 
                    rename(sites = 'n()')

result <- left_join(result, sites_per_year)

```

Make graphs

```{r}
cols                <- c('Non-London' = 'red', 'LAEIBox' = 'blue')

ggplot(data = filter(result, pollutant %in% c('pm10', 'pm25', 'no2') & capture_rate > 70), aes(date, mean, group=code)) +
  geom_path(alpha=0.4,aes(colour=name)) +
  scale_color_manual(values = cols) +
  #geom_smooth(data = filter(result, pollutant %in% c('pm10', 'pm25', 'no2') & percentage > 70), aes(date, mean, group=1),
  #            colour = 'white', fill = 'white') +
  facet_wrap(.~toupper(pollutant), scales = 'free_y', ncol = 1) +
  theme(legend.position   = 'none',
        panel.grid        = element_blank(),
        panel.background  = element_rect(fill = 'black'),
        axis.title.x      = element_blank(),
        strip.background  = element_blank(),
        strip.text.x      = element_text(angle = 0, hjust = 0, colour='black', size=12),
        axis.text         = element_text(size = 12, colour = 'black')) +
 ylab (expression(paste("Concentration (", mu, g, "/", m^3, ")", sep=""))) +
  transition_reveal(date) +
  ease_aes('linear')
```

London sites compared to non-London sites?